{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "example_librispeech_training.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "OHy3V7O_lu-d",
        "Ld5EZj6_mnH5",
        "Zx-hzi6-o0ze"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vectominist/MiniASR/blob/main/example/example_librispeech_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc6s2gOOlg-W"
      },
      "source": [
        "# **MiniASR Tutorial: LibriSpeech Training**\n",
        "This is a tutorial for training an end-to-end automatic speech recognition model with the toolkit [MiniASR](https://github.com/vectominist/MiniASR).  \n",
        "You can run this notebook on [Google Colab](colab.research.google.com/), but to train an ASR model completely requires a Pro account since it needs several hours to converge."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHy3V7O_lu-d"
      },
      "source": [
        "## **Download Code & Install Dependencies**\n",
        "Ref: [MiniASR](https://github.com/vectominist/MiniASR)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3ZxMQbvk25k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "823cfbe1-6b79-4ce7-87b6-45aebbaf6495"
      },
      "source": [
        "! git clone https://github.com/vectominist/MiniASR.git\n",
        "% cd MiniASR"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MiniASR'...\n",
            "remote: Enumerating objects: 161, done.\u001b[K\n",
            "remote: Counting objects: 100% (161/161), done.\u001b[K\n",
            "remote: Compressing objects: 100% (94/94), done.\u001b[K\n",
            "remote: Total 161 (delta 71), reused 140 (delta 54), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (161/161), 134.92 KiB | 8.99 MiB/s, done.\n",
            "Resolving deltas: 100% (71/71), done.\n",
            "/content/MiniASR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf7QLclNieZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18581a77-b0c3-4d12-9d09-cc9d8573af60"
      },
      "source": [
        "! pip3 install -e ./"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Obtaining file:///content/MiniASR\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from miniasr==0.1) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.7/dist-packages (from miniasr==0.1) (1.19.5)\n",
            "Collecting sentencepiece>=0.1.96\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 8.0 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning>=1.3.8\n",
            "  Downloading pytorch_lightning-1.4.9-py3-none-any.whl (925 kB)\n",
            "\u001b[K     |████████████████████████████████| 925 kB 42.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (from miniasr==0.1) (1.9)\n",
            "Requirement already satisfied: joblib>=0.12.4 in /usr/local/lib/python3.7/dist-packages (from miniasr==0.1) (1.0.1)\n",
            "Requirement already satisfied: librosa>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from miniasr==0.1) (0.8.1)\n",
            "Collecting numba==0.48\n",
            "  Downloading numba-0.48.0-1-cp37-cp37m-manylinux2014_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 56.4 MB/s \n",
            "\u001b[?25hCollecting edit_distance\n",
            "  Downloading edit_distance-1.0.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from miniasr==0.1) (1.9.0+cu102)\n",
            "Collecting torchaudio>=0.7.0\n",
            "  Downloading torchaudio-0.9.1-cp37-cp37m-manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 50.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from miniasr==0.1) (0.10.0+cu102)\n",
            "Requirement already satisfied: torchtext>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from miniasr==0.1) (0.10.0)\n",
            "Collecting llvmlite<0.32.0,>=0.31.0dev0\n",
            "  Downloading llvmlite-0.31.0-cp37-cp37m-manylinux1_x86_64.whl (20.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 20.2 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba==0.48->miniasr==0.1) (57.4.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->miniasr==0.1) (0.22.2.post1)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->miniasr==0.1) (0.10.3.post1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->miniasr==0.1) (21.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->miniasr==0.1) (1.5.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->miniasr==0.1) (2.1.9)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->miniasr==0.1) (0.2.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->miniasr==0.1) (1.4.1)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa>=0.7.2->miniasr==0.1) (4.4.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa>=0.7.2->miniasr==0.1) (2.4.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.7.2->miniasr==0.1) (2.23.0)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa>=0.7.2->miniasr==0.1) (1.4.4)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2021.9.0-py3-none-any.whl (123 kB)\n",
            "\u001b[K     |████████████████████████████████| 123 kB 45.3 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 53.3 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.3.8->miniasr==0.1) (3.7.4.3)\n",
            "Collecting PyYAML>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 41.1 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.0\n",
            "  Downloading torchmetrics-0.5.1-py3-none-any.whl (282 kB)\n",
            "\u001b[K     |████████████████████████████████| 282 kB 56.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning>=1.3.8->miniasr==0.1) (2.6.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 42.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa>=0.7.2->miniasr==0.1) (1.15.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa>=0.7.2->miniasr==0.1) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa>=0.7.2->miniasr==0.1) (2.20)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (1.40.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (0.37.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (3.3.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (0.4.6)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (0.12.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (0.6.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (1.8.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (4.7.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa>=0.7.2->miniasr==0.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa>=0.7.2->miniasr==0.1) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa>=0.7.2->miniasr==0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa>=0.7.2->miniasr==0.1) (2021.5.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (3.1.1)\n",
            "Collecting torch>=1.7.0\n",
            "  Downloading torch-1.9.1-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 6.7 kB/s \n",
            "\u001b[?25hCollecting torchtext>=0.8.0\n",
            "  Downloading torchtext-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 25.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.8.0->miniasr==0.1) (7.1.2)\n",
            "Collecting torchvision>=0.8.0\n",
            "  Downloading torchvision-0.10.1-cp37-cp37m-manylinux1_x86_64.whl (22.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.1 MB 50.8 MB/s \n",
            "\u001b[?25hCollecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[K     |████████████████████████████████| 294 kB 50.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning>=1.3.8->miniasr==0.1) (21.2.0)\n",
            "Collecting async-timeout<4.0,>=3.0\n",
            "  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[K     |████████████████████████████████| 142 kB 47.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning>=1.3.8->miniasr==0.1) (3.5.0)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=f6ac296a7082a9925adbbb65ba23839734107885d5cf7ed9642ed497f8be9d96\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "Successfully built future\n",
            "Installing collected packages: multidict, yarl, llvmlite, async-timeout, torch, numba, fsspec, aiohttp, torchmetrics, PyYAML, pyDeprecate, future, torchvision, torchtext, torchaudio, sentencepiece, pytorch-lightning, edit-distance, miniasr\n",
            "  Attempting uninstall: llvmlite\n",
            "    Found existing installation: llvmlite 0.34.0\n",
            "    Uninstalling llvmlite-0.34.0:\n",
            "      Successfully uninstalled llvmlite-0.34.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.9.0+cu102\n",
            "    Uninstalling torch-1.9.0+cu102:\n",
            "      Successfully uninstalled torch-1.9.0+cu102\n",
            "  Attempting uninstall: numba\n",
            "    Found existing installation: numba 0.51.2\n",
            "    Uninstalling numba-0.51.2:\n",
            "      Successfully uninstalled numba-0.51.2\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.10.0+cu102\n",
            "    Uninstalling torchvision-0.10.0+cu102:\n",
            "      Successfully uninstalled torchvision-0.10.0+cu102\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.10.0\n",
            "    Uninstalling torchtext-0.10.0:\n",
            "      Successfully uninstalled torchtext-0.10.0\n",
            "  Running setup.py develop for miniasr\n",
            "Successfully installed PyYAML-5.4.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 edit-distance-1.0.4 fsspec-2021.9.0 future-0.18.2 llvmlite-0.31.0 miniasr-0.1 multidict-5.1.0 numba-0.48.0 pyDeprecate-0.3.1 pytorch-lightning-1.4.9 sentencepiece-0.1.96 torch-1.9.1 torchaudio-0.9.1 torchmetrics-0.5.1 torchtext-0.10.1 torchvision-0.10.1 yarl-1.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld5EZj6_mnH5"
      },
      "source": [
        "## **Download Data**\n",
        "- training set: [Libri-light](https://github.com/facebookresearch/libri-light) fine-tuning set (10 hours, 0.6G)\n",
        "- development set: [LibriSpeech](https://www.openslr.org/12) `dev-clean` set\n",
        "- testing set: [LibriSpeech](https://www.openslr.org/12) `test-clean` set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Nmh3oywnAgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e2a931-8afa-484c-ef91-4e9ca5aabe87"
      },
      "source": [
        "! mkdir -p data\n",
        "% cd data\n",
        "! wget https://dl.fbaipublicfiles.com/librilight/data/librispeech_finetuning.tgz\n",
        "! tar zxf librispeech_finetuning.tgz\n",
        "! rm librispeech_finetuning.tgz"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/MiniASR/data\n",
            "--2021-10-01 08:36:17--  https://dl.fbaipublicfiles.com/librilight/data/librispeech_finetuning.tgz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 104.22.74.142, 172.67.9.4, 104.22.75.142, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|104.22.74.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 597601132 (570M) [application/gzip]\n",
            "Saving to: ‘librispeech_finetuning.tgz’\n",
            "\n",
            "librispeech_finetun 100%[===================>] 569.92M  36.4MB/s    in 17s     \n",
            "\n",
            "2021-10-01 08:36:35 (32.7 MB/s) - ‘librispeech_finetuning.tgz’ saved [597601132/597601132]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6BIVK1knUTK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a514c638-bfe9-4dba-e309-c2db055652ba"
      },
      "source": [
        "! wget https://www.openslr.org/resources/12/dev-clean.tar.gz\n",
        "! wget https://www.openslr.org/resources/12/test-clean.tar.gz\n",
        "! tar zxf dev-clean.tar.gz\n",
        "! tar zxf test-clean.tar.gz\n",
        "! rm dev-clean.tar.gz\n",
        "! rm test-clean.tar.gz\n",
        "% cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-01 08:36:41--  https://www.openslr.org/resources/12/dev-clean.tar.gz\n",
            "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
            "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 337926286 (322M) [application/x-gzip]\n",
            "Saving to: ‘dev-clean.tar.gz’\n",
            "\n",
            "dev-clean.tar.gz    100%[===================>] 322.27M  18.0MB/s    in 19s     \n",
            "\n",
            "2021-10-01 08:37:00 (17.3 MB/s) - ‘dev-clean.tar.gz’ saved [337926286/337926286]\n",
            "\n",
            "--2021-10-01 08:37:00--  https://www.openslr.org/resources/12/test-clean.tar.gz\n",
            "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
            "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 346663984 (331M) [application/x-gzip]\n",
            "Saving to: ‘test-clean.tar.gz’\n",
            "\n",
            "test-clean.tar.gz   100%[===================>] 330.60M  18.0MB/s    in 19s     \n",
            "\n",
            "2021-10-01 08:37:20 (17.3 MB/s) - ‘test-clean.tar.gz’ saved [346663984/346663984]\n",
            "\n",
            "/content/MiniASR\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx-hzi6-o0ze"
      },
      "source": [
        "## **Preprocess Data**\n",
        "Find all data in the corpus and extract vocabularies. We use characters as text tokens since the dataset is small."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWWcTzXho4b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40fa9113-d55b-4ef6-f2ec-6a4e4547f414"
      },
      "source": [
        "# Train set\n",
        "! miniasr-preprocess \\\n",
        "        -c LibriSpeech \\\n",
        "        -p data/librispeech_finetuning \\\n",
        "        -s 1h \\\n",
        "        -o data/libri_train_1h \\\n",
        "        --gen-vocab \\\n",
        "        --char-vocab-size 40\n",
        "\n",
        "! miniasr-preprocess \\\n",
        "        -c LibriSpeech \\\n",
        "        -p data/librispeech_finetuning \\\n",
        "        -s 9h \\\n",
        "        -o data/libri_train_9h\n",
        "\n",
        "# Development set\n",
        "! miniasr-preprocess \\\n",
        "        -c LibriSpeech \\\n",
        "        -p data/LibriSpeech \\\n",
        "        -s dev-clean \\\n",
        "        -o data/libri_dev\n",
        "\n",
        "# Test set\n",
        "! miniasr-preprocess \\\n",
        "        -c LibriSpeech \\\n",
        "        -p data/LibriSpeech \\\n",
        "        -s test-clean \\\n",
        "        -o data/libri_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-01 08:37 run_preprocess.py.main(72) Preprocessing LibriSpeech corpus.\n",
            "10-01 08:37 run_preprocess.py.main(73) Subsets = ['1h']\n",
            "10-01 08:37 run_preprocess.py.main(76) Results will be saved to data/libri_train_1h\n",
            "10-01 08:37 run_preprocess.py.main(79) Reading data from data/librispeech_finetuning\n",
            "10-01 08:37 run_preprocess.py.main(85) Found 286 audio files.\n",
            "10-01 08:37 run_preprocess.py.main(89) Saving unsorted data dict to data/libri_train_1h/data_dict.json\n",
            "10-01 08:37 run_preprocess.py.main(94) Sorting data by audio file length.\n",
            "10-01 08:37 run_preprocess.py.main(103) Saving sorted data list to data/libri_train_1h/data_list_sorted.json\n",
            "10-01 08:37 run_preprocess.py.main(109) Generating LM file.\n",
            "10-01 08:37 run_preprocess.py.main(117) Generating vocabularies.\n",
            "10-01 08:37 run_preprocess.py.main(120) Generating characters.\n",
            "Found 28 chars.\n",
            "Selected 28 vocabularies.\n",
            "Saving char vocabularies to data/libri_train_1h/vocab_char.txt\n",
            "10-01 08:37 run_preprocess.py.main(72) Preprocessing LibriSpeech corpus.\n",
            "10-01 08:37 run_preprocess.py.main(73) Subsets = ['9h']\n",
            "10-01 08:37 run_preprocess.py.main(76) Results will be saved to data/libri_train_9h\n",
            "10-01 08:37 run_preprocess.py.main(79) Reading data from data/librispeech_finetuning\n",
            "10-01 08:37 run_preprocess.py.main(85) Found 2477 audio files.\n",
            "10-01 08:37 run_preprocess.py.main(89) Saving unsorted data dict to data/libri_train_9h/data_dict.json\n",
            "10-01 08:37 run_preprocess.py.main(94) Sorting data by audio file length.\n",
            "10-01 08:37 run_preprocess.py.main(103) Saving sorted data list to data/libri_train_9h/data_list_sorted.json\n",
            "10-01 08:37 run_preprocess.py.main(109) Generating LM file.\n",
            "10-01 08:37 run_preprocess.py.main(72) Preprocessing LibriSpeech corpus.\n",
            "10-01 08:37 run_preprocess.py.main(73) Subsets = ['dev-clean']\n",
            "10-01 08:37 run_preprocess.py.main(76) Results will be saved to data/libri_dev\n",
            "10-01 08:37 run_preprocess.py.main(79) Reading data from data/LibriSpeech\n",
            "10-01 08:37 run_preprocess.py.main(85) Found 2703 audio files.\n",
            "10-01 08:37 run_preprocess.py.main(89) Saving unsorted data dict to data/libri_dev/data_dict.json\n",
            "10-01 08:37 run_preprocess.py.main(94) Sorting data by audio file length.\n",
            "10-01 08:37 run_preprocess.py.main(103) Saving sorted data list to data/libri_dev/data_list_sorted.json\n",
            "10-01 08:37 run_preprocess.py.main(109) Generating LM file.\n",
            "10-01 08:37 run_preprocess.py.main(72) Preprocessing LibriSpeech corpus.\n",
            "10-01 08:37 run_preprocess.py.main(73) Subsets = ['test-clean']\n",
            "10-01 08:37 run_preprocess.py.main(76) Results will be saved to data/libri_test\n",
            "10-01 08:37 run_preprocess.py.main(79) Reading data from data/LibriSpeech\n",
            "10-01 08:37 run_preprocess.py.main(85) Found 2620 audio files.\n",
            "10-01 08:37 run_preprocess.py.main(89) Saving unsorted data dict to data/libri_test/data_dict.json\n",
            "10-01 08:37 run_preprocess.py.main(94) Sorting data by audio file length.\n",
            "10-01 08:37 run_preprocess.py.main(103) Saving sorted data list to data/libri_test/data_list_sorted.json\n",
            "10-01 08:37 run_preprocess.py.main(109) Generating LM file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcyY1IWMrezU"
      },
      "source": [
        "## **Training**\n",
        "- Modify `MiniASR/egs/librispeech/config/ctc_train_example.yaml` for changing training hyper-parameters.\n",
        "- The results will be saved to `MiniASR/model/ctc_libri-10h_char`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDENJR7Wq7pN"
      },
      "source": [
        "! mkdir -p model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTBUATPSsXsh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e09ddbd8-d41d-459f-f769-739aacfe09de"
      },
      "source": [
        "! minasr-asr --config egs/librispeech/config/ctc_train_example.yaml\n",
        "\n",
        "# Resume training with this command:\n",
        "# ! minasr-asr --ckpt model/ctc_libri-10h_char/epoch=4-step=429.ckpt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-01 12:12 run_asr.py.main(99) Training mode.\n",
            "10-01 12:12 dataloader.py.create_dataloader(86) Creating text tokenizer of character level.\n",
            "10-01 12:12 dataloader.py.create_dataloader(91) Generating datasets and dataloaders. (mode = train)\n",
            "10-01 12:12 dataset.py.__init__(28) Loading data from ['data/libri_train_1h/data_list_sorted.json', 'data/libri_train_9h/data_list_sorted.json']\n",
            "100% 2763/2763 [00:00<00:00, 12494.93it/s]\n",
            "10-01 12:12 dataset.py.__init__(49) 2763 audio files found (mode = train)\n",
            "10-01 12:12 dataset.py.__init__(28) Loading data from ['data/libri_dev/data_list_sorted.json']\n",
            "100% 2703/2703 [00:00<00:00, 21381.83it/s]\n",
            "10-01 12:12 dataset.py.__init__(49) 2703 audio files found (mode = dev)\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "10-01 12:12 asr_trainer.py.create_asr_trainer(24) Creating ASR model (type = ctc_asr).\n",
            "[s3prl.upstream.experts] Warning: can not import s3prl.upstream.wav2vec.expert: No module named 'fairseq'. Pass.\n",
            "[s3prl.upstream.experts] Warning: can not import s3prl.upstream.wav2vec2_hug.expert: No module named 'transformers'. Pass.\n",
            "[s3prl.upstream.experts] Warning: can not import s3prl.upstream.roberta.expert: No module named 'fairseq'. Pass.\n",
            "[s3prl.upstream.experts] Warning: can not import s3prl.upstream.vq_wav2vec.expert: No module named 'fairseq'. Pass.\n",
            "[s3prl.upstream.experts] Warning: can not import s3prl.upstream.wav2vec2.expert: No module named 'fairseq'. Pass.\n",
            "[s3prl.upstream.experts] Warning: can not import s3prl.upstream.hubert.expert: No module named 'fairseq'. Pass.\n",
            "[s3prl.upstream.experts] Warning: can not import s3prl.upstream.decoar2.expert: No module named 'fairseq'. Pass.\n",
            "[s3prl.hub] Warning: can not import s3prl.upstream.wav2vec.hubconf: No module named 'fairseq'. Please see upstream/wav2vec/README.md\n",
            "[s3prl.hub] Warning: can not import s3prl.upstream.wav2vec2_hug.hubconf: No module named 'transformers'. Please see upstream/wav2vec2_hug/README.md\n",
            "[s3prl.hub] Warning: can not import s3prl.upstream.roberta.hubconf: No module named 'fairseq'. Please see upstream/roberta/README.md\n",
            "[s3prl.hub] Warning: can not import s3prl.upstream.vq_wav2vec.hubconf: No module named 'fairseq'. Please see upstream/vq_wav2vec/README.md\n",
            "[s3prl.hub] Warning: can not import s3prl.upstream.wav2vec2.hubconf: No module named 'fairseq'. Please see upstream/wav2vec2/README.md\n",
            "[s3prl.hub] Warning: can not import s3prl.upstream.hubert.hubconf: No module named 'fairseq'. Please see upstream/hubert/README.md\n",
            "[s3prl.hub] Warning: can not import s3prl.upstream.decoar2.hubconf: No module named 'fairseq'. Please see upstream/decoar2/README.md\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.enhancement_stft.expert: No module named 'asteroid'. Pass.\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.quesst14_dtw.expert: No module named 'dtw'. Pass.\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.speech_translation.expert: No module named 'sacrebleu'. Pass.\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.asr.expert: No module named 'fairseq'. Pass.\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.voxceleb2_ge2e.expert: No module named 'sox'. Pass.\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.sv_voxceleb1.expert: No module named 'sox'. Pass.\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.separation_stft.expert: No module named 'asteroid'. Pass.\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.speech_commands.expert: No module named 'catalyst'. Pass.\n",
            "Using native 16bit precision.\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name             | Type             | Params\n",
            "------------------------------------------------------\n",
            "0 | extractor        | UpstreamExpert   | 0     \n",
            "1 | feat_select      | FeatureSelection | 0     \n",
            "2 | encoder          | RNNEncoder       | 3.1 M \n",
            "3 | ctc_output_layer | Linear           | 15.9 K\n",
            "4 | ctc_loss         | CTCLoss          | 0     \n",
            "------------------------------------------------------\n",
            "3.1 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.1 M     Total params\n",
            "12.585    Total estimated model params size (MB)\n",
            "Validation sanity check:   0% 0/2 [00:00<?, ?it/s]\n",
            "10-01 12:12 base_asr.py.validation_epoch_end(219) Val CER = 92.9% , WER = 100.0% , Loss = 18.55\n",
            "Epoch 4:  58% 100/171 [01:25<01:00,  1.18it/s, loss=2.79, v_num=0]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/85 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  70% 120/171 [01:36<00:40,  1.25it/s, loss=2.79, v_num=0]\n",
            "Epoch 4:  82% 140/171 [01:42<00:22,  1.38it/s, loss=2.79, v_num=0]\n",
            "Epoch 4:  94% 160/171 [01:47<00:07,  1.50it/s, loss=2.79, v_num=0]\n",
            "Validating:  94% 80/85 [00:24<00:01,  3.75it/s]\u001b[A\n",
            "Validating: 100% 85/85 [00:25<00:00,  4.04it/s]\u001b[A\n",
            "10-01 12:20 base_asr.py.validation_epoch_end(219) Val CER = 84.4% , WER = 104.3% , Loss = 2.80\n",
            "Epoch 4: 100% 171/171 [02:03<00:00,  1.39it/s, loss=2.78, v_num=0]\n",
            "Epoch 9:  58% 100/171 [01:25<01:00,  1.18it/s, loss=2.29, v_num=0]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/85 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  70% 120/171 [01:36<00:40,  1.25it/s, loss=2.29, v_num=0]\n",
            "Epoch 9:  82% 140/171 [01:42<00:22,  1.37it/s, loss=2.29, v_num=0]\n",
            "Epoch 9:  94% 160/171 [01:47<00:07,  1.50it/s, loss=2.29, v_num=0]\n",
            "Validating:  94% 80/85 [00:25<00:01,  3.76it/s]\u001b[A\n",
            "Epoch 9: 100% 171/171 [01:58<00:00,  1.45it/s, loss=2.29, v_num=0]\n",
            "10-01 12:28 base_asr.py.validation_epoch_end(219) Val CER = 70.5% , WER = 111.0% , Loss = 2.22\n",
            "Epoch 9: 100% 171/171 [02:18<00:00,  1.24it/s, loss=2.29, v_num=0]\n",
            "Epoch 14:  58% 100/171 [01:25<01:00,  1.18it/s, loss=1.98, v_num=0]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/85 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  70% 120/171 [01:36<00:40,  1.25it/s, loss=1.98, v_num=0]\n",
            "Epoch 14:  82% 140/171 [01:42<00:22,  1.38it/s, loss=1.98, v_num=0]\n",
            "Epoch 14:  94% 160/171 [01:46<00:07,  1.51it/s, loss=1.98, v_num=0]\n",
            "Validating:  94% 80/85 [00:24<00:01,  3.79it/s]\u001b[A\n",
            "Epoch 14: 100% 171/171 [02:06<00:00,  1.36it/s, loss=1.98, v_num=0]\n",
            "10-01 12:36 base_asr.py.validation_epoch_end(219) Val CER = 57.3% , WER = 102.2% , Loss = 1.89\n",
            "Epoch 14: 100% 171/171 [02:31<00:00,  1.13it/s, loss=2, v_num=0]   \n",
            "Epoch 19:  58% 100/171 [01:26<01:00,  1.17it/s, loss=1.78, v_num=0]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/85 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  70% 120/171 [01:37<00:40,  1.25it/s, loss=1.78, v_num=0]\n",
            "Epoch 19:  82% 140/171 [01:43<00:22,  1.37it/s, loss=1.78, v_num=0]\n",
            "Epoch 19:  94% 160/171 [01:47<00:07,  1.49it/s, loss=1.78, v_num=0]\n",
            "Validating:  94% 80/85 [00:25<00:01,  3.76it/s]\u001b[A\n",
            "Epoch 19: 100% 171/171 [02:01<00:00,  1.42it/s, loss=1.78, v_num=0]\n",
            "10-01 12:45 base_asr.py.validation_epoch_end(219) Val CER = 53.0% , WER = 96.2% , Loss = 1.70\n",
            "Epoch 19: 100% 171/171 [02:33<00:00,  1.12it/s, loss=1.78, v_num=0]\n",
            "Epoch 24:  58% 100/171 [01:25<01:00,  1.18it/s, loss=1.63, v_num=0]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/85 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  70% 120/171 [01:36<00:40,  1.25it/s, loss=1.63, v_num=0]\n",
            "Epoch 24:  82% 140/171 [01:42<00:22,  1.37it/s, loss=1.63, v_num=0]\n",
            "Epoch 24:  94% 160/171 [01:47<00:07,  1.50it/s, loss=1.63, v_num=0]\n",
            "Validating:  94% 80/85 [00:25<00:01,  3.75it/s]\u001b[A\n",
            "Epoch 24: 100% 171/171 [02:03<00:00,  1.39it/s, loss=1.63, v_num=0]\n",
            "10-01 12:53 base_asr.py.validation_epoch_end(219) Val CER = 48.5% , WER = 98.7% , Loss = 1.56\n",
            "Epoch 24: 100% 171/171 [02:37<00:00,  1.09it/s, loss=1.63, v_num=0]\n",
            "Epoch 29:  58% 100/171 [01:26<01:00,  1.17it/s, loss=1.51, v_num=0]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/85 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  70% 120/171 [01:37<00:40,  1.25it/s, loss=1.51, v_num=0]\n",
            "Epoch 29:  82% 140/171 [01:42<00:22,  1.37it/s, loss=1.51, v_num=0]\n",
            "Epoch 29:  94% 160/171 [01:47<00:07,  1.50it/s, loss=1.51, v_num=0]\n",
            "Validating:  94% 80/85 [00:25<00:01,  3.77it/s]\u001b[A\n",
            "Epoch 29: 100% 171/171 [02:01<00:00,  1.41it/s, loss=1.51, v_num=0]\n",
            "10-01 13:02 base_asr.py.validation_epoch_end(219) Val CER = 45.9% , WER = 94.4% , Loss = 1.47\n",
            "Epoch 29: 100% 171/171 [02:39<00:00,  1.08it/s, loss=1.53, v_num=0]\n",
            "Epoch 34:  58% 100/171 [01:26<01:00,  1.17it/s, loss=1.42, v_num=0]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/85 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  70% 120/171 [01:37<00:40,  1.25it/s, loss=1.42, v_num=0]\n",
            "Epoch 34:  82% 140/171 [01:43<00:22,  1.37it/s, loss=1.42, v_num=0]\n",
            "Epoch 34:  94% 160/171 [01:47<00:07,  1.50it/s, loss=1.42, v_num=0]\n",
            "Validating:  94% 80/85 [00:24<00:01,  3.79it/s]\u001b[A\n",
            "Epoch 34: 100% 171/171 [01:57<00:00,  1.46it/s, loss=1.42, v_num=0]\n",
            "10-01 13:10 base_asr.py.validation_epoch_end(219) Val CER = 43.2% , WER = 90.8% , Loss = 1.38\n",
            "Epoch 34: 100% 171/171 [02:39<00:00,  1.08it/s, loss=1.41, v_num=0]\n",
            "Epoch 39:  58% 100/171 [01:25<01:00,  1.18it/s, loss=1.35, v_num=0]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 39:  58% 100/171 [01:35<01:07,  1.06it/s, loss=1.35, v_num=0]\n",
            "Epoch 39:  70% 120/171 [01:36<00:40,  1.25it/s, loss=1.35, v_num=0]\n",
            "Epoch 39:  82% 140/171 [01:42<00:22,  1.37it/s, loss=1.35, v_num=0]\n",
            "Epoch 39:  94% 160/171 [01:47<00:07,  1.50it/s, loss=1.35, v_num=0]\n",
            "Validating:  94% 80/85 [00:25<00:01,  3.76it/s]\u001b[A\n",
            "Epoch 39: 100% 171/171 [02:05<00:00,  1.37it/s, loss=1.35, v_num=0]\n",
            "10-01 13:18 base_asr.py.validation_epoch_end(219) Val CER = 41.0% , WER = 89.7% , Loss = 1.34\n",
            "Epoch 39: 100% 171/171 [02:42<00:00,  1.06it/s, loss=1.35, v_num=0]\n",
            "Epoch 44:  58% 100/171 [01:25<00:59,  1.18it/s, loss=1.26, v_num=0]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/85 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  70% 120/171 [01:36<00:40,  1.26it/s, loss=1.26, v_num=0]\n",
            "Epoch 44:  82% 140/171 [01:42<00:22,  1.38it/s, loss=1.26, v_num=0]\n",
            "Epoch 44:  94% 160/171 [01:46<00:07,  1.51it/s, loss=1.26, v_num=0]\n",
            "Validating:  94% 80/85 [00:24<00:01,  3.78it/s]\u001b[A\n",
            "Epoch 44: 100% 171/171 [02:01<00:00,  1.41it/s, loss=1.26, v_num=0]\n",
            "10-01 13:27 base_asr.py.validation_epoch_end(219) Val CER = 39.7% , WER = 86.5% , Loss = 1.28\n",
            "Epoch 44: 100% 171/171 [02:40<00:00,  1.07it/s, loss=1.28, v_num=0]\n",
            "Epoch 45:  47% 40/86 [00:54<01:01,  1.34s/it, loss=1.29, v_num=0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVXy9YUVuLpt"
      },
      "source": [
        "## **Testing**\n",
        "- Specify your checkpoint with `--ckpt`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3B1mCCKuOYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe244d16-6eeb-469a-b663-80afd04f3e01"
      },
      "source": [
        "! minasr-asr \\\n",
        "    --config egs/librispeech/config/ctc_test_example.yaml \\\n",
        "    --test \\\n",
        "    --override \"args.data.dev_paths=['data/libri_test/data_list_sorted.json']\" \\\n",
        "    --ckpt model/ctc_libri-10h_char/epoch=44-step=3869.ckpt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10-01 13:28 basic_setups.py.override(89) Override: args.data.dev_paths = ['data/libri_test/data_list_sorted.json']\n",
            "10-01 13:28 run_asr.py.main(105) Testing mode.\n",
            "[s3prl.upstream.experts] Warning: can not import s3prl.upstream.wav2vec.expert: No module named 'fairseq'. Pass.\n",
            "[s3prl.upstream.experts] Warning: can not import s3prl.upstream.wav2vec2_hug.expert: No module named 'transformers'. Pass.\n",
            "[s3prl.upstream.experts] Warning: can not import s3prl.upstream.roberta.expert: No module named 'fairseq'. Pass.\n",
            "[s3prl.upstream.experts] Warning: can not import s3prl.upstream.vq_wav2vec.expert: No module named 'fairseq'. Pass.\n",
            "[s3prl.upstream.experts] Warning: can not import s3prl.upstream.wav2vec2.expert: No module named 'fairseq'. Pass.\n",
            "[s3prl.upstream.experts] Warning: can not import s3prl.upstream.hubert.expert: No module named 'fairseq'. Pass.\n",
            "[s3prl.upstream.experts] Warning: can not import s3prl.upstream.decoar2.expert: No module named 'fairseq'. Pass.\n",
            "[s3prl.hub] Warning: can not import s3prl.upstream.wav2vec.hubconf: No module named 'fairseq'. Please see upstream/wav2vec/README.md\n",
            "[s3prl.hub] Warning: can not import s3prl.upstream.wav2vec2_hug.hubconf: No module named 'transformers'. Please see upstream/wav2vec2_hug/README.md\n",
            "[s3prl.hub] Warning: can not import s3prl.upstream.roberta.hubconf: No module named 'fairseq'. Please see upstream/roberta/README.md\n",
            "[s3prl.hub] Warning: can not import s3prl.upstream.vq_wav2vec.hubconf: No module named 'fairseq'. Please see upstream/vq_wav2vec/README.md\n",
            "[s3prl.hub] Warning: can not import s3prl.upstream.wav2vec2.hubconf: No module named 'fairseq'. Please see upstream/wav2vec2/README.md\n",
            "[s3prl.hub] Warning: can not import s3prl.upstream.hubert.hubconf: No module named 'fairseq'. Please see upstream/hubert/README.md\n",
            "[s3prl.hub] Warning: can not import s3prl.upstream.decoar2.hubconf: No module named 'fairseq'. Please see upstream/decoar2/README.md\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.enhancement_stft.expert: No module named 'asteroid'. Pass.\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.quesst14_dtw.expert: No module named 'dtw'. Pass.\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.speech_translation.expert: No module named 'sacrebleu'. Pass.\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.asr.expert: No module named 'fairseq'. Pass.\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.voxceleb2_ge2e.expert: No module named 'sox'. Pass.\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.sv_voxceleb1.expert: No module named 'sox'. Pass.\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.separation_stft.expert: No module named 'asteroid'. Pass.\n",
            "[s3prl.downstream.experts] Warning: can not import s3prl.downstream.speech_commands.expert: No module named 'catalyst'. Pass.\n",
            "10-01 13:28 dataloader.py.create_dataloader(91) Generating datasets and dataloaders. (mode = dev)\n",
            "10-01 13:28 dataset.py.__init__(28) Loading data from ['data/libri_test/data_list_sorted.json']\n",
            "100% 2620/2620 [00:00<00:00, 21892.29it/s]\n",
            "10-01 13:28 dataset.py.__init__(49) 2620 audio files found (mode = dev)\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "Testing: 100% 164/164 [00:39<00:00,  8.80it/s]\n",
            "\n",
            "Character errors\n",
            "| #Snt     #Tok     | Sub    Del    Ins    Err    SErr   |\n",
            "| 2620     281530   | 16.7   21.1   1.5    39.2   100.0  |\n",
            "\n",
            "Word errors\n",
            "| #Snt     #Tok     | Sub    Del    Ins    Err    SErr   |\n",
            "| 2620     52576    | 74.7   7.2    4.4    86.4   100.0  |\n",
            "\n",
            "RTF:     0.0013\n",
            "Latency: 9.3666 [ms/sentence]\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "DATALOADER:0 TEST RESULTS\n",
            "{}\n",
            "--------------------------------------------------------------------------------\n",
            "Testing: 100% 164/164 [01:19<00:00,  2.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTjPqeq-EIh8"
      },
      "source": [
        "## **Inference**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99pGiNgrRNSZ"
      },
      "source": [
        "from miniasr.utils import load_from_checkpoint, sequence_distance\n",
        "from miniasr.data.audio import load_waveform\n",
        "\n",
        "model, args, tokenizer = load_from_checkpoint(\n",
        "    'model/ctc_libri-10h_char/epoch=44-step=3869.ckpt', 'cuda')\n",
        "waves = [load_waveform('data/LibriSpeech/dev-clean/6345/93302/6345-93302-0025.flac').to('cuda')]\n",
        "hyps = model.recognize(waves)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UazYyxmZSKOQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b56a5cc-6e36-41a6-a730-75eeb0558717"
      },
      "source": [
        "print(hyps[0])\n",
        "ref = 'ARE YOU REALLY GOING TO THROW ME OVER FOR A THING LIKE THIS'\n",
        "res_cer = sequence_distance(ref, hyps[0], mode='char')\n",
        "res_wer = sequence_distance(ref, hyps[0], mode='word')\n",
        "print('CER = {:.2f}%'.format(100. * res_cer['distance'] / res_cer['length']))\n",
        "print('WER = {:.2f}%'.format(100. * res_wer['distance'] / res_wer['length']))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Y WILY O T OE ME R THING MY FES\n",
            "CER = 59.32%\n",
            "WER = 84.62%\n"
          ]
        }
      ]
    }
  ]
}